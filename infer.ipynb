{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/swv2/lib/python3.11/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from tools.models.mynet import FusionNet\n",
    "import numpy as np\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "import lpips\n",
    "valid_transform = A.Compose([ToTensorV2()])\n",
    "\n",
    "\n",
    "def compute_ssim(img1, img2):\n",
    "    \"\"\"\n",
    "    计算两幅灰度图像的 SSIM 指数\n",
    "    (输入图像应为相同尺寸的 numpy 数组，建议先转换为灰度图)\n",
    "    \"\"\"\n",
    "    # 检查图像尺寸是否相同\n",
    "    if img1.shape != img2.shape:\n",
    "        raise ValueError(\"Input images must have the same dimensions\")\n",
    "\n",
    "    # 转换为浮点数计算\n",
    "    img1 = img1.astype(np.float32)\n",
    "    img2 = img2.astype(np.float32)\n",
    "\n",
    "    # 常量参数 (基于 8-bit 图像动态范围 0-255)\n",
    "    C1 = (0.01 * 255) ** 2\n",
    "    C2 = (0.03 * 255) ** 2\n",
    "\n",
    "    # 高斯核参数\n",
    "    kernel_size = (11, 11)\n",
    "    sigma = 1.5\n",
    "\n",
    "    # 计算均值 (高斯模糊)\n",
    "    mu1 = cv2.GaussianBlur(img1, kernel_size, sigma)\n",
    "    mu2 = cv2.GaussianBlur(img2, kernel_size, sigma)\n",
    "\n",
    "    # 计算均值平方\n",
    "    mu1_sq = mu1**2\n",
    "    mu2_sq = mu2**2\n",
    "    mu1_mu2 = mu1 * mu2\n",
    "\n",
    "    # 计算方差和协方差\n",
    "    sigma1_sq = cv2.GaussianBlur(img1**2, kernel_size, sigma) - mu1_sq\n",
    "    sigma2_sq = cv2.GaussianBlur(img2**2, kernel_size, sigma) - mu2_sq\n",
    "    sigma12 = cv2.GaussianBlur(img1 * img2, kernel_size, sigma) - mu1_mu2\n",
    "\n",
    "    # SSIM 计算\n",
    "    numerator = (2 * mu1_mu2 + C1) * (2 * sigma12 + C2)\n",
    "    denominator = (mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2)\n",
    "    ssim_map = numerator / denominator\n",
    "\n",
    "    return np.mean(ssim_map)\n",
    "\n",
    "\n",
    "def calculate_metrics(pred_dir, gt_dir):\n",
    "    \"\"\"\n",
    "    计算预测图像和真实图像之间的 PSNR, SSIM, RMSE 和 LPIPS。\n",
    "\n",
    "    Args:\n",
    "        pred_dir (str): 预测图像目录。\n",
    "        gt_dir (str): 真实图像目录。\n",
    "\n",
    "    Returns:\n",
    "        dict: 包含平均 PSNR, SSIM, RMSE 和 LPIPS 值的字典。\n",
    "    \"\"\"\n",
    "\n",
    "    pred_files = [f for f in os.listdir(pred_dir) if f.lower().endswith(\".bmp\")]\n",
    "    gt_files = [f for f in os.listdir(gt_dir) if f.lower().endswith(\".bmp\")]\n",
    "    common_files = sorted(\n",
    "        list(set(pred_files) & set(gt_files))\n",
    "    )  # Find common files and sort\n",
    "\n",
    "    if not common_files:\n",
    "        print(\"No common BMP files found between the two directories.\")\n",
    "        return None\n",
    "\n",
    "    psnr_values = []\n",
    "    ssim_values = []\n",
    "    rmse_values = []\n",
    "    lpips_alex_values = []\n",
    "    lpips_vgg_values = []\n",
    "    lpips_squeeze_values = []\n",
    "\n",
    "    # Initialize LPIPS loss functions\n",
    "    loss_fn_alex = lpips.LPIPS(net=\"alex\").to(\n",
    "        \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    )\n",
    "    loss_fn_vgg = lpips.LPIPS(net=\"vgg\").to(\n",
    "        \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    )\n",
    "    loss_fn_squeeze = lpips.LPIPS(net=\"squeeze\").to(\n",
    "        \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    )\n",
    "\n",
    "    for filename in common_files:\n",
    "        pred_path = os.path.join(pred_dir, filename)\n",
    "        gt_path = os.path.join(gt_dir, filename)\n",
    "\n",
    "        try:\n",
    "            pred_img = cv2.imread(pred_path, cv2.IMREAD_COLOR)\n",
    "            gt_img = cv2.imread(gt_path, cv2.IMREAD_COLOR)\n",
    "\n",
    "            if pred_img is None or gt_img is None:\n",
    "                print(f\"Skipping {filename} due to read error.\")\n",
    "                continue\n",
    "\n",
    "            if pred_img.shape != gt_img.shape:\n",
    "                print(f\"Skipping {filename} due to the different image shape\")\n",
    "                continue\n",
    "\n",
    "            # Convert images to grayscale for SSIM (scikit-image's ssim expects grayscale)\n",
    "            pred_gray = cv2.cvtColor(pred_img, cv2.COLOR_BGR2GRAY)\n",
    "            gt_gray = cv2.cvtColor(gt_img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            # Calculate PSNR\n",
    "            psnr_values.append(psnr(gt_img, pred_img, data_range=255))\n",
    "\n",
    "            # Calculate SSIM\n",
    "            ssim_values.append(compute_ssim(gt_gray, pred_gray))\n",
    "\n",
    "            # Calculate RMSE\n",
    "            rmse = np.sqrt(\n",
    "                np.mean(\n",
    "                    (\n",
    "                        gt_img.astype(np.float64) / 255.0\n",
    "                        - pred_img.astype(np.float64) / 255.0\n",
    "                    )\n",
    "                    ** 2\n",
    "                )\n",
    "            )\n",
    "            rmse_values.append(rmse)\n",
    "\n",
    "            # Calculate LPIPS (requires [0,1] normalized, torch Tensor images)\n",
    "            gt_img_tensor = (\n",
    "                torch.from_numpy(gt_img).permute(2, 0, 1).unsqueeze(0).float()\n",
    "            ) / 127.5 - 1\n",
    "            pred_img_tensor = (\n",
    "                torch.from_numpy(pred_img).permute(2, 0, 1).unsqueeze(0).float()\n",
    "            ) / 127.5 - 1\n",
    "\n",
    "            # Move tensors to the same device as the loss functions\n",
    "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "            gt_img_tensor = gt_img_tensor.to(device)\n",
    "            pred_img_tensor = pred_img_tensor.to(device)\n",
    "\n",
    "            lpips_alex = loss_fn_alex(gt_img_tensor, pred_img_tensor).item()\n",
    "            lpips_vgg = loss_fn_vgg(gt_img_tensor, pred_img_tensor).item()\n",
    "            lpips_squeeze = loss_fn_squeeze(gt_img_tensor, pred_img_tensor).item()\n",
    "\n",
    "            lpips_alex_values.append(lpips_alex)\n",
    "            lpips_vgg_values.append(lpips_vgg)\n",
    "            lpips_squeeze_values.append(lpips_squeeze)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "            continue\n",
    "\n",
    "    if not psnr_values:  # Check if any images were successfully processed\n",
    "        print(\"No images were successfully processed.\")\n",
    "        return None\n",
    "\n",
    "    # Calculate average metrics\n",
    "    avg_psnr = np.mean(psnr_values)\n",
    "    avg_ssim = np.mean(ssim_values)\n",
    "    avg_rmse = np.mean(rmse_values)\n",
    "    avg_lpips_alex = np.mean(lpips_alex_values)\n",
    "    avg_lpips_vgg = np.mean(lpips_vgg_values)\n",
    "    avg_lpips_squeeze = np.mean(lpips_squeeze_values)\n",
    "\n",
    "    return {\n",
    "        \"PSNR\": avg_psnr,\n",
    "        \"SSIM\": avg_ssim,\n",
    "        \"RMSE\": avg_rmse,\n",
    "        \"LPIPS_alex\": avg_lpips_alex,\n",
    "        \"LPIPS_vgg\": avg_lpips_vgg,\n",
    "        \"LPIPS_squeeze\": avg_lpips_squeeze,\n",
    "    }\n",
    "\n",
    "\n",
    "def perform_inference(\n",
    "    config,\n",
    "    ckpt_path,\n",
    "    test_dir=\"/media/hdd/sonwe1e/Competition/PBVS_Thermal/Data/valid/CUBIC_x8\",\n",
    "    out_dir=None,\n",
    "    device=0,\n",
    "    num_ckpts=1,  # 新增参数：选择checkpoint的数量\n",
    "    select_by=\"max_loss\",  # 新增参数：选择方式（max_loss/min_loss）\n",
    "):\n",
    "    \"\"\"\n",
    "    Performs inference with multiple checkpoints and TTA.\n",
    "\n",
    "    Args:\n",
    "        num_ckpts (int): Number of checkpoints to use (sorted by loss)\n",
    "        select_by (str): 'max_loss' or 'min_loss' to specify checkpoint selection\n",
    "    \"\"\"\n",
    "    if out_dir is None:\n",
    "        out_dir = os.path.join(os.path.dirname(test_dir), \"PRED\")\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    # 1. 解析checkpoint文件 ------------------------------------------------\n",
    "    ckpt_files = [f for f in os.listdir(ckpt_path) if f.endswith(\".ckpt\")]\n",
    "    if not ckpt_files:\n",
    "        raise ValueError(f\"No checkpoint found in {ckpt_path}\")\n",
    "\n",
    "    # 提取loss值并排序\n",
    "    parsed = []\n",
    "    for f in ckpt_files:\n",
    "        match = re.search(r\"loss_([0-9]+\\.[0-9]+)\", f)\n",
    "        if match:\n",
    "            loss = float(match.group(1))\n",
    "            parsed.append((f, loss))\n",
    "\n",
    "    if not parsed:\n",
    "        raise ValueError(\"No valid checkpoint files with loss in name\")\n",
    "\n",
    "    # 根据选择方式排序\n",
    "    reverse = True if select_by == \"max_loss\" else False\n",
    "    parsed_sorted = sorted(parsed, key=lambda x: x[1], reverse=reverse)\n",
    "    selected_files = parsed_sorted[:num_ckpts]\n",
    "\n",
    "    # 2. 加载多个模型 -----------------------------------------------------\n",
    "    models = []\n",
    "    for fname, loss in selected_files:\n",
    "        model = FusionNet(\n",
    "            dim=config.dim,\n",
    "            n_blocks=config.n_blocks,\n",
    "            upscaling_factor=config.upscaling_factor\n",
    "        ).cuda(device)\n",
    "\n",
    "        # 加载checkpoint\n",
    "        ckpt = torch.load(\n",
    "            os.path.join(ckpt_path, fname),\n",
    "            map_location=f\"cuda:{device}\",\n",
    "            weights_only=False,\n",
    "        )[\"state_dict\"]\n",
    "\n",
    "        # 处理key\n",
    "        for k in list(ckpt.keys()):\n",
    "            new_key = k.replace(\"model.\", \"\")\n",
    "            ckpt[new_key] = ckpt.pop(k)\n",
    "            if \"loss\" in new_key:\n",
    "                del ckpt[new_key]\n",
    "\n",
    "        model.load_state_dict(ckpt, strict=True)\n",
    "        model.eval()\n",
    "        models.append(model)\n",
    "\n",
    "    # 3. 执行推理（多模型+多TTA）---------------------------------------------\n",
    "    for img_name in tqdm(os.listdir(test_dir)):\n",
    "        if not img_name.endswith(\".bmp\"):\n",
    "            continue\n",
    "\n",
    "        # 加载图像\n",
    "        img_path = os.path.join(test_dir, img_name)\n",
    "        img = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # 预处理\n",
    "        orig = valid_transform(image=img)[\"image\"] / 127.5 - 1\n",
    "\n",
    "        # 创建四种TTA变换\n",
    "        images = [\n",
    "            orig,\n",
    "            torch.flip(orig, [2]),  # 水平翻转\n",
    "            torch.flip(orig, [1]),  # 垂直翻转\n",
    "            torch.flip(orig, [1, 2]),  # 水平+垂直翻转\n",
    "        ]\n",
    "\n",
    "        all_preds = []\n",
    "        for model in models:\n",
    "            model_preds = []\n",
    "            for idx, inp in enumerate(images):\n",
    "                inp_tensor = inp.unsqueeze(0).cuda(device)\n",
    "                with torch.no_grad():\n",
    "                    output = model(inp_tensor)\n",
    "\n",
    "                # 逆变换\n",
    "                if idx == 1:\n",
    "                    output = torch.flip(output, [3])\n",
    "                elif idx == 2:\n",
    "                    output = torch.flip(output, [2])\n",
    "                elif idx == 3:\n",
    "                    output = torch.flip(output, [2, 3])\n",
    "\n",
    "                model_preds.append(output)\n",
    "\n",
    "            # 单模型的多TTA平均\n",
    "            model_avg = torch.mean(torch.stack(model_preds), dim=0)\n",
    "            all_preds.append(model_avg)\n",
    "\n",
    "        # 多模型平均\n",
    "        final_pred = torch.mean(torch.stack(all_preds), dim=0)\n",
    "        # final_pred = all_preds[0]\n",
    "\n",
    "        # 后处理并保存\n",
    "        final_pred = final_pred.squeeze()\n",
    "        final_pred = (\n",
    "            (final_pred.permute(1, 2, 0).cpu().numpy() * 127.5 + 127.5)\n",
    "            .clip(0, 255)\n",
    "            .astype(\"uint8\")\n",
    "        )\n",
    "        cv2.imwrite(os.path.join(out_dir, img_name), final_pred)\n",
    "\n",
    "    # 4. 压缩结果 ---------------------------------------------------------\n",
    "    print(\"Zipping results...\")\n",
    "    zip_name = f\"PRED_{os.path.basename(ckpt_path)}_top{num_ckpts}_{select_by}.zip\"\n",
    "    os.system(f\"cd {out_dir} && zip '{os.path.join('../../../PBVS', zip_name)}' *.bmp\")\n",
    "    print(f\"Results saved to: {out_dir}\")\n",
    "    return out_dir\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'perform_inference' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 10\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupscaling_factor \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m\n\u001b[1;32m      9\u001b[0m opt \u001b[38;5;241m=\u001b[39m SMFANetConfig()\n\u001b[0;32m---> 10\u001b[0m \u001b[43mperform_inference\u001b[49m(\n\u001b[1;32m     11\u001b[0m     opt,\n\u001b[1;32m     12\u001b[0m     ckpt_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./checkpoints/v10+lkfb_attn_sigmoid\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     13\u001b[0m     num_ckpts\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m     14\u001b[0m     select_by\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     15\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'perform_inference' is not defined"
     ]
    }
   ],
   "source": [
    "# 示例配置\n",
    "class SMFANetConfig:\n",
    "    def __init__(self):\n",
    "        self.dim = 96\n",
    "        self.n_blocks = 8\n",
    "        self.upscaling_factor = 8\n",
    "\n",
    "\n",
    "opt = SMFANetConfig()\n",
    "perform_inference(\n",
    "    opt,\n",
    "    ckpt_path=\"./checkpoints/v10+lkfb_attn_sigmoid\",\n",
    "    num_ckpts=3,\n",
    "    select_by=\"max_loss\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/swv2/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/miniconda3/envs/swv2/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /home/ubuntu/miniconda3/envs/swv2/lib/python3.11/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/swv2/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /home/ubuntu/miniconda3/envs/swv2/lib/python3.11/site-packages/lpips/weights/v0.1/vgg.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [squeeze], v[0.1], spatial [off]\n",
      "Loading model from: /home/ubuntu/miniconda3/envs/swv2/lib/python3.11/site-packages/lpips/weights/v0.1/squeeze.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/swv2/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=SqueezeNet1_1_Weights.IMAGENET1K_V1`. You can also use `weights=SqueezeNet1_1_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metrics:\n",
      "PSNR: 27.2405\n",
      "SSIM: 0.8272\n",
      "RMSE: 0.0453\n",
      "LPIPS_alex: 0.2837\n",
      "LPIPS_vgg: 0.3636\n",
      "LPIPS_squeeze: 0.2465\n"
     ]
    }
   ],
   "source": [
    "pred_dir = \"/media/hdd/sonwe1e/Competition/PBVS_Thermal/Data/valid/PRED\"\n",
    "gt_dir = \"/media/hdd/sonwe1e/Competition/PBVS_Thermal/Data/valid/GT\"\n",
    "\n",
    "metrics = calculate_metrics(pred_dir, gt_dir)\n",
    "\n",
    "if metrics:\n",
    "    print(f\"Average Metrics:\")\n",
    "    for key, value in metrics.items():\n",
    "        print(f\"{key}: {value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
